{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow/Keras transfer learning example\n",
    "\n",
    "This notebook gives a quick introduction to using a CNN trained on imagenet to do classification on a different problem.\n",
    "\n",
    "In this case, we take VGG16 (because it's simple) and re-train it to classify handwritten digits from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical, model_to_dot\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the pretrained model, just to have a look at it\n",
    "pretrained_model = VGG16()\n",
    "\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it expects a given input size, and produces a prediction based on the number of classes it was trained on.\n",
    "\n",
    "Now lets try loading the VGG network with our custom options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_model = VGG16(include_top=False, weights='imagenet', input_shape=(32,32,3), classes=10)\n",
    "retraining_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is no densely connected layer at the end now, this is just the feature extraction bits of the network\n",
    "\n",
    "Now we add our own classification parts, namely a densely connected layer with 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened = Flatten()(retraining_model.output)\n",
    "fc1 = Dense(4096, activation='relu')(flattened)\n",
    "fc2 = Dense(10, activation='softmax')(fc1)\n",
    "\n",
    "mnist_model = Model(inputs=retraining_model.input, outputs=fc2)\n",
    "mnist_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This line visualises the VGG_MNIST network graph\n",
    "display(Image(model_to_dot(mnist_model).create(prog='dot', format='png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start training the network! However, we have a couple of things to do:\n",
    "- We should freeze some layers in the pretrained part of the model (how many is up to you)\n",
    "- We need to load the dataset and tweak it a bit to work with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we grab up to the last 4 layers (block5_conv{1,2,3} and block5_pool) and set \n",
    "## them to be untrainable, leaving only the last few layers\n",
    "for layer in retraining_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "mnist_model.summary() ## Note how the number of trainable params has gone down a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can load the MNIST data\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = mnist.load_data()\n",
    "print(x_train_raw.shape)\n",
    "\n",
    "## Pad around the images with zeros to make them 32x32\n",
    "x_train = np.pad(x_train_raw, ((0,0), (2,2),(2,2)), 'constant', constant_values=0)\n",
    "x_test = np.pad(x_test_raw, ((0,0), (2,2),(2,2)), 'constant', constant_values=0)\n",
    "\n",
    "## make into 3 channel\n",
    "x_train = x_train[:,:,:, None] * np.ones(3)[None, None, None, :]\n",
    "x_test = x_test[:,:,:, None] * np.ones(3)[None, None, None, :]\n",
    "\n",
    "\n",
    "## convert integer labels to categorical\n",
    "y_train = to_categorical(y_train_raw)\n",
    "y_test = to_categorical(y_test_raw)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready, we can re-train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n",
    "history = mnist_model.fit(x_train[:4096], y_train[:4096], epochs=5, batch_size=32, validation_split=0.2) ## NB this is deliberately short so it might run in time!\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the model using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.evaluate(x_test[:100], y_test[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now unfreeze a few more layers in the network and use an optimizer with a small learning rate to fine tune the network a bit more until we were happy with performance.\n",
    "\n",
    "That's left as an exercise for the reader!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
